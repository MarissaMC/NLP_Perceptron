CSCI 544 Homework 2
=====================
This is an instruction for CSCI544 Homework 2(http://appliednlp.bitbucket.org/hw2/index.html)

The code written for Python 3.4.


Part I
-----------------
The Averaged Perceptron classifier can be used as below:

    python3 perceplearn.py TRAININGFILE MODEL
	python3 percepclassify.py MODEL < TESTINGFILE > OUTPUT

Both TRAININGFILE and TESTINGFILE should have the same format as below:

TRAININGFILE:

LABEL_1 FEATURE_11 FEATURE_12 ... FEATURE_1N 
LABEL_2 FEATURE_21 FEATURE_22 ... FEATURE_2N 
... 
LABEL_M FEATURE_M1 FEATURE_M2 ... FEATURE_MN 

TESTINGFILE

FEATURE_11 FEATURE_12 ... FEATURE_1N 
FEATURE_21 FEATURE_22 ... FEATURE_2N 
... 
FEATURE_M1 FEATURE_M2 ... FEATURE_MN 

Part II
-----------------
Use my averaged perceptron to perform part-of-speech tagging.

The training program, postrain, run as follows:

    python3 postrain.py TRAININGFILE MODEL

where TRAININGFILE is the input file formated with one sentence per line, and each sentence composed of word/tag pairs. For example, a small training file might contain these lines:

This/DT is/VBZ a/DT test/NN ./.

I/PRP saw/VBD a/DT movie/NN ./.

I/PRP like/VBP cookies/NNS ./.


and MODEL is the output file containing the model.

The postag program runs as follows:

    python3 postag.py MODEL < TESTINGFILE > OUTPUT

where MODEL is the model generated by postrain.

postag takes its input from STDIN in the form of one sentence per line,
where each sentence is a sequence of words (without tags). Output will be written to STDOUT, 
and will be a tagged sentence (in the same format as the training data) for each input sentence.

Part III
-----------------
Use my averaged perceptron to perform named entity recogntion.

The method to use it is similar to Part II:

    python3 nelearn.py TRAININGFILE MODEL
	python3 netag.py MODEL < TESTINGFILE > OUTPUT
    
The NER data format is similar to the POS tagging data format, 
but also includes a POS tag between each word and its NER BIO tag: 

WORD/POSTAG/NERTAG WORD/POSTAG/NERTAG ...

Part IV
-----------------
###Source of information
Piazza discussion, TA office hour

### Accuracy of my part-of-speech tagger
After 5 iteration of Averaged Perceptron, my POS tagger accuracy is 93.9%

### Precision, recall and F-score for each named entity types and the overall F-score
After 5 iteration of Averaged Perceptron, I got the following information:

B-LOC

Precision: 0.6052871467639015

Recall: 0.6747967479674797

F-score: 0.6381547333012973

B-PER

Precision: 0.4516276937184778

Recall: 0.8060556464811784

F-score: 0.578900969732589

B-MISC

Precision: 0.6507352941176471

Recall: 0.39775280898876403

F-score: 0.4937238493723849

B-ORG

Precision: 0.7228831350594822

Recall: 0.6076470588235294

F-score: 0.6602748481943113

I-LOC

Precision: 0.5899581589958159

Recall: 0.41839762611275966

F-score: 0.4895833333333333

I-PER

Precision: 0.84593837535014

Recall: 0.35157159487776485

F-score: 0.49671052631578955

I-MISC

Precision: 0.5833333333333334

Recall: 0.25688073394495414

F-score: 0.356687898089172

I-ORG

Precision: 0.6071428571428571

Recall: 0.46046852122986826

F-score: 0.5237302248126561

O

Precision: 0.9771206327133669

Recall: 0.9915115971426052

F-score: 0.9842635149923398

The overall F-score: 0.93

### Use my Naive Bayes classifier instead of my perceptron classifier

Use the Naive Bayes I wrote in HW1 as classifier:

For POS dataset, my POS tagger accuracy is 94.2%

For NER dataset, the performance metrics are as below:

I-LOC

Precision: 0.7204301075268817

Recall: 0.39762611275964393

F-score: 0.5124282982791587

B-LOC

Precision: 0.668769716088328

Recall: 0.6463414634146342

F-score: 0.6573643410852713

I-PER

Precision: 0.8535211267605634

Recall: 0.35273573923166474

F-score: 0.49917627677100496

B-ORG

Precision: 0.7221022318214543

Recall: 0.59

F-score: 0.6494011006798316

I-MISC

Precision: 0.6631578947368421

Recall: 0.1926605504587156

F-score: 0.2985781990521327

B-PER

Precision: 0.9280575539568345

Recall: 0.31669394435351883

F-score: 0.4722391702257474

B-MISC

Precision: 0.7844311377245509

Recall: 0.2943820224719101

F-score: 0.4281045751633986

O

Precision: 0.9751077070368598

Recall: 0.8533159890642914

F-score: 0.9101555610427176

I-ORG

Precision: 0.7365269461077845

Recall: 0.3601756954612006

F-score: 0.48377581120943947

The overall F-score: 0.832

We can see that my Naive Bayes POS tagger has accuracy rate of 94.2%, which is a little higher than my Averaged Perceptron accuracy rate 93.9%. I think one of the reason is may because of my poor Perceptron quality. Also, because POS dataset has as many as 45 classes, which will make finding the class by referring from previous and next words a difficult thing. From my point of view, Perceptron is more sensitive than Naive Bayes for it changes from step by step. If I am unlucky that I meet with many special cases at the end of iteration of Averaged Perceptron, it will affect my model relatively heavily, which may also cause a fall in accuracy rate.

When using Naive Bayes to tag NER dataset, we can see a relatively lower Precision, Recall and F-score value than my Averaged Perceptron. I think it is because the major class in NER dataset is "O", which means the classes distribution is not even. As Naive Bayes cares about probability, when P("O") is much bigger than other classes, word has class "O" will tend to be tagged with "O". This may cause tag mistake when a word has low occurrence in all its classes and has class "O". But Perceptron doesn't have this problem because it corrects the weight step by step.